{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97d3e90-78d4-45c2-9ac0-77fb7afb67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install pandas\n",
    "#!pip3 install PyArrow\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc05b817-b158-4e24-be1b-eff1fc97932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9742f40b-e5e5-40e8-8f62-692a985e953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe2eab2d-04db-43bd-9ccc-f9a6f6ff2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"/etc/hadoop/conf/hive-site.xml\"):\n",
    "    tree = ET.parse(\"/etc/hadoop/conf/hive-site.xml\")\n",
    "    root = tree.getroot()\n",
    "    for prop in root.findall(\"property\"):\n",
    "        if prop.find(\"name\").text == \"hive.metastore.warehouse.dir\":\n",
    "            storage = (\n",
    "                prop.find(\"value\").text.split(\"/\")[0]\n",
    "                + \"//\"\n",
    "                + prop.find(\"value\").text.split(\"/\")[2]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "362747fd-6e30-43c6-948a-94e3751f1b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"STORAGE\"] = storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbe32b7-5801-47b0-8f3b-a81aa6fb8fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://go01-demo\n"
     ]
    }
   ],
   "source": [
    "print(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64146b81-faaa-43ea-93a0-ce58cb23c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"PythonSQL\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-2\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",os.environ[\"STORAGE\"])\\\n",
    "    .config(\"spark.rpc.message.maxSize\", \"1024\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .getOrCreate()\n",
    " #   .config(\"spark.driver.cores\", 4)\\\n",
    " #   .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "de006039-1f0d-4d60-86b2-f1eab7d4d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = spark.read.csv(os.environ[\"STORAGE\"]+'/cde-workshop/clickthrough/customers/data', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "12bcff34-f662-497b-b1f2-98367dd5259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers_df.select(\"customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6176b8f-3cf3-4a07-b621-0c9561d1fad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in ./.local/lib/python3.7/site-packages (15.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /runtime-addons/cmladdon-python-2.0.32-b123/opt/cmladdons/python/site-packages (from faker) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.1 in /runtime-addons/cmladdon-python-2.0.32-b123/opt/cmladdons/python/site-packages (from faker) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /runtime-addons/cmladdon-python-2.0.32-b123/opt/cmladdons/python/site-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0a04c92f-410b-469e-a765-b3f961045e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer_id', 'string'),\n",
       " ('username', 'string'),\n",
       " ('name', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('email', 'string'),\n",
       " ('occupation', 'string'),\n",
       " ('birthdate', 'string'),\n",
       " ('address', 'string'),\n",
       " ('device_id', 'string')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a43fe9c-637d-470d-b655-f39e4ba8af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_current_cust_id = int(customers_df.select(F.max(\"customer_id\")).collect()[0]['max(customer_id)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cf197be-18ee-48ba-b908-91ceac9ecbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047fb6fd-0833-4da9-9beb-ad7a870bedd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker(seed=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351313b8-0e46-45ae-9109-3308894e5679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.local/lib/python3.7/site-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /runtime-addons/cmladdon-python-2.0.32-b123/opt/cmladdons/python/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in /runtime-addons/cmladdon-python-2.0.32-b123/opt/cmladdons/python/site-packages (from pandas) (2022.4)\n",
      "Requirement already satisfied: six>=1.5 in /runtime-addons/cmladdon-python-2.0.32-b123/opt/cmladdons/python/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "486b9fec-a2f8-4129-97ae-e818f6aa1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "appends = []\n",
    "for i in range(5):\n",
    "    appends.append(fake.profile())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bf386e5-dc30-498a-bb1d-4bba752a3871",
   "metadata": {},
   "outputs": [],
   "source": [
    "appends_df = pd.DataFrame(appends)[[\"username\", \"name\", \"sex\", \"mail\", \"job\", \"birthdate\", \"address\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d1665af-05cb-4126-a37f-6fdbd3f79d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01902c93-9670-46ff-bd15-3071411b2f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_df(max_current_cust_id):\n",
    "    \n",
    "    batch_size = random.randint(100, 1000)\n",
    "    appends = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        appends.append(fake.profile())\n",
    "        \n",
    "    appends_df = pd.DataFrame(appends)[[\"username\", \"name\", \"sex\", \"mail\", \"job\", \"birthdate\", \"address\"]]\n",
    "    appends_df['customer_id'] = list(range(max_current_cust_id, max_current_cust_id+batch_size))\n",
    "    \n",
    "    appends_df.rename({'sex': 'gender', 'job': 'position', 'mail':'email'}, axis=1)\n",
    "    \n",
    "    return appends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99f21acc-e429-427e-85a1-875f898c884c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>mail</th>\n",
       "      <th>job</th>\n",
       "      <th>birthdate</th>\n",
       "      <th>address</th>\n",
       "      <th>customer_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mavila</td>\n",
       "      <td>Rhonda Johnson</td>\n",
       "      <td>F</td>\n",
       "      <td>johnwilliams@yahoo.com</td>\n",
       "      <td>Therapist, drama</td>\n",
       "      <td>2000-09-12</td>\n",
       "      <td>Unit 3313 Box 5140\\nDPO AA 02879</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kevin36</td>\n",
       "      <td>Carmen Griffin</td>\n",
       "      <td>F</td>\n",
       "      <td>dalemullins@yahoo.com</td>\n",
       "      <td>Trade mark attorney</td>\n",
       "      <td>1946-04-15</td>\n",
       "      <td>991 Lauren Fort\\nMossville, WA 57733</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jpineda</td>\n",
       "      <td>Allen Torres</td>\n",
       "      <td>M</td>\n",
       "      <td>kevinjenkins@gmail.com</td>\n",
       "      <td>Water engineer</td>\n",
       "      <td>1972-09-09</td>\n",
       "      <td>3400 Charles Plain Apt. 285\\nMorganbury, SC 04316</td>\n",
       "      <td>10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>krystalellis</td>\n",
       "      <td>Barbara Mitchell</td>\n",
       "      <td>F</td>\n",
       "      <td>jamesrios@gmail.com</td>\n",
       "      <td>Soil scientist</td>\n",
       "      <td>1908-02-10</td>\n",
       "      <td>2788 Castillo Fall\\nRebeccaview, HI 20055</td>\n",
       "      <td>10002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hancockrichard</td>\n",
       "      <td>George Parker</td>\n",
       "      <td>M</td>\n",
       "      <td>imooney@yahoo.com</td>\n",
       "      <td>Scientist, research (maths)</td>\n",
       "      <td>1996-03-03</td>\n",
       "      <td>1549 Conway Valleys\\nSouth Johnfort, AL 43825</td>\n",
       "      <td>10003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>qrice</td>\n",
       "      <td>Robert Hernandez</td>\n",
       "      <td>M</td>\n",
       "      <td>brandon09@gmail.com</td>\n",
       "      <td>Waste management officer</td>\n",
       "      <td>1912-05-13</td>\n",
       "      <td>2263 Tina Springs\\nNew Melissa, IN 52637</td>\n",
       "      <td>10353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>djones</td>\n",
       "      <td>Kristi Henderson</td>\n",
       "      <td>F</td>\n",
       "      <td>isabelhowe@gmail.com</td>\n",
       "      <td>Heritage manager</td>\n",
       "      <td>1911-02-26</td>\n",
       "      <td>429 Desiree Stream\\nNew Jessicashire, SD 68761</td>\n",
       "      <td>10354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>williamsjustin</td>\n",
       "      <td>Jeffrey Anderson</td>\n",
       "      <td>M</td>\n",
       "      <td>jcordova@yahoo.com</td>\n",
       "      <td>Translator</td>\n",
       "      <td>1937-01-13</td>\n",
       "      <td>43439 Hodge Motorway\\nMillerstad, NC 70033</td>\n",
       "      <td>10355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>nsilva</td>\n",
       "      <td>Jennifer Jackson</td>\n",
       "      <td>F</td>\n",
       "      <td>ifoley@hotmail.com</td>\n",
       "      <td>Dietitian</td>\n",
       "      <td>1918-05-27</td>\n",
       "      <td>5876 Brittany Cove\\nNguyenbury, DE 38422</td>\n",
       "      <td>10356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>juliemcgee</td>\n",
       "      <td>Michael Bridges</td>\n",
       "      <td>M</td>\n",
       "      <td>tmahoney@gmail.com</td>\n",
       "      <td>Manufacturing systems engineer</td>\n",
       "      <td>1927-09-07</td>\n",
       "      <td>41263 Greer Islands\\nRyanshire, ND 34432</td>\n",
       "      <td>10357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           username              name sex                    mail  \\\n",
       "0            mavila    Rhonda Johnson   F  johnwilliams@yahoo.com   \n",
       "1           kevin36    Carmen Griffin   F   dalemullins@yahoo.com   \n",
       "2           jpineda      Allen Torres   M  kevinjenkins@gmail.com   \n",
       "3      krystalellis  Barbara Mitchell   F     jamesrios@gmail.com   \n",
       "4    hancockrichard     George Parker   M       imooney@yahoo.com   \n",
       "..              ...               ...  ..                     ...   \n",
       "354           qrice  Robert Hernandez   M     brandon09@gmail.com   \n",
       "355          djones  Kristi Henderson   F    isabelhowe@gmail.com   \n",
       "356  williamsjustin  Jeffrey Anderson   M      jcordova@yahoo.com   \n",
       "357          nsilva  Jennifer Jackson   F      ifoley@hotmail.com   \n",
       "358      juliemcgee   Michael Bridges   M      tmahoney@gmail.com   \n",
       "\n",
       "                                job   birthdate  \\\n",
       "0                  Therapist, drama  2000-09-12   \n",
       "1               Trade mark attorney  1946-04-15   \n",
       "2                    Water engineer  1972-09-09   \n",
       "3                    Soil scientist  1908-02-10   \n",
       "4       Scientist, research (maths)  1996-03-03   \n",
       "..                              ...         ...   \n",
       "354        Waste management officer  1912-05-13   \n",
       "355                Heritage manager  1911-02-26   \n",
       "356                      Translator  1937-01-13   \n",
       "357                       Dietitian  1918-05-27   \n",
       "358  Manufacturing systems engineer  1927-09-07   \n",
       "\n",
       "                                               address  customer_id  \n",
       "0                     Unit 3313 Box 5140\\nDPO AA 02879         9999  \n",
       "1                 991 Lauren Fort\\nMossville, WA 57733        10000  \n",
       "2    3400 Charles Plain Apt. 285\\nMorganbury, SC 04316        10001  \n",
       "3            2788 Castillo Fall\\nRebeccaview, HI 20055        10002  \n",
       "4        1549 Conway Valleys\\nSouth Johnfort, AL 43825        10003  \n",
       "..                                                 ...          ...  \n",
       "354           2263 Tina Springs\\nNew Melissa, IN 52637        10353  \n",
       "355     429 Desiree Stream\\nNew Jessicashire, SD 68761        10354  \n",
       "356         43439 Hodge Motorway\\nMillerstad, NC 70033        10355  \n",
       "357           5876 Brittany Cove\\nNguyenbury, DE 38422        10356  \n",
       "358           41263 Greer Islands\\nRyanshire, ND 34432        10357  \n",
       "\n",
       "[359 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_batch_df(9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e20a1bf1-c5fa-4041-833e-a1e45b641078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ct_hist_df = spark.read.option(\"header\",\"true\").parquet(\"s3a://go01-demo/cde-workshop/clickthrough/historical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "004f2271-7c03-4455-bae6-794565b9c6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hist_ded_ids = ct_hist_df.select(\"device_id\").sample(.01).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3ba4dc52-828d-4e24-8b78-d243e29c213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_arr = np.unique(hist_ded_ids.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fd01dd54-2522-4c65-a0a9-06aecf0a2878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000032d7', '000070cc', '00038618', ..., 'ffff60f9', 'ffff9249',\n",
       "       'ffffe321'], dtype=object)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ec662aca-0db8-4f68-ab3e-6b6fdb648533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_ded_ids = unique_arr[0:10000]\n",
    "len(hist_ded_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2db616f-9e6d-45b4-9066-54c41428dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_pd_df = customers_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d76e0adf-fe29-4674-90e2-2a78faac3e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_id    10000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist_ded_ids.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c50cd89f-db56-4e48-9ead-0899b6a5976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#customers_pd_df = customers_pd_df.drop([\"devide_id\"])\n",
    "customers_pd_df[\"device_id\"] = hist_ded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "47f08182-0cc7-4441-919f-a6a134d9cd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_pd_df = customers_pd_df.drop(\"devide_id\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83830dce-d219-4ea3-b5d6-7171c19b0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_spark_df_customers = spark.createDataFrame(customers_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "23d14a30-37dc-486a-83a5-46338c136a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_spark_df_customers.write.mode(\"overwrite\").csv(os.environ[\"STORAGE\"]+'/cde-workshop/clickthrough/customers/data', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2fce4a16-9099-47ee-a1a6-21448a9950e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "verify_df = spark.read.csv(os.environ[\"STORAGE\"]+'/cde-workshop/clickthrough/customers/data', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9904396-42e6-470a-951c-b5cd98f793c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------------------+------+--------------------+--------------------+----------+--------------------+---------+\n",
      "|customer_id|      username|                name|gender|               email|          occupation| birthdate|             address|device_id|\n",
      "+-----------+--------------+--------------------+------+--------------------+--------------------+----------+--------------------+---------+\n",
      "|          1|       robin48|       Jesse Spencer|     M|   udalton@yahoo.com|Pharmacist, commu...|1975-09-24|10305 Scott River...| 000032d7|\n",
      "|          2|cynthiajackson|     Savannah Daniel|     F|walkerchristopher...|        Set designer|1934-09-28|70884 Andrew Plaz...| 000070cc|\n",
      "|          3|       ydurham|     Alexander Davis|     M|annlindsey@yahoo.com|Plant breeder/gen...|1975-11-09|0365 Carrie Point...| 00038618|\n",
      "|          4| murphymichael|      Patrick Cortez|     M| freeves@hotmail.com|Scientist, audiol...|1911-01-20|9864 Brian Walk S...| 000727b1|\n",
      "|          5|    brittany85|      Robert Ballard|     M|donald73@hotmail.com|    Industrial buyer|2010-08-12|PSC 2637, Box 266...| 000828f7|\n",
      "|          6|       kevin51|    Gary Watkins DDS|     M|     jay41@gmail.com|Air traffic contr...|1941-03-29|2983 Stewart Cres...| 0008e3ee|\n",
      "|          7|thomasanderson|  Elizabeth Harrison|     F|  cwalsh@hotmail.com|Pharmacist, commu...|1926-03-31|369 Jillian Circl...| 0009e271|\n",
      "|          8|         fcole|        Thomas Simon|     M|wyattshannon@hotm...|            Animator|1941-09-11|36165 Amanda Cres...| 000b6e10|\n",
      "|          9|      wbennett|       Thomas Rivers|     M|    ggates@yahoo.com|Scientist, audiol...|1936-07-20|8254 Jennifer Poi...| 00102b65|\n",
      "|         10|    danielkent|        Mariah Rojas|     F|  dawnruiz@gmail.com|Exhibitions offic...|1957-02-20|189 Bryan Mountai...| 00171260|\n",
      "|         11|  joneskristin|       Dale Williams|     M|      ulee@yahoo.com|            Best boy|1924-11-27|79823 Jonathan Ov...| 0019f7d0|\n",
      "|         12|        sbrown|Mrs. Shelley Lambert|     F|  todddunn@gmail.com|Commercial/reside...|1969-03-20|88079 Brian WallM...| 001ac871|\n",
      "|         13|     michael81|    Melissa Castillo|     F|michaeldavila@gma...|Journalist, broad...|1930-02-12|94427 Rachel KeyH...| 001dd02b|\n",
      "|         14| lambchristian|         Lisa Kelley|     F|rebeccasmith@hotm...|Consulting civil ...|2003-04-29|617 Pena PointJen...| 001fa78e|\n",
      "|         15|        ssmith|       Willie Medina|     M|  cjenkins@yahoo.com|Higher education ...|1922-10-20|PSC 4133, Box 951...| 0024ee8d|\n",
      "|         16|       jason58|     Benjamin Torres|     M|schmidtsue@yahoo.com|        Risk analyst|1993-07-19|2404 Roger Club A...| 00262b95|\n",
      "|         17|      kjohnson|          Mark Davis|     M|connerwilliam@yah...| Associate Professor|1938-10-31|68487 Villegas Ma...| 00268f31|\n",
      "|         18|   walkersarah|    Christina Ingram|     F|craigfaulkner@hot...|Magazine features...|1918-05-28|10893 Prince Fore...| 0026921d|\n",
      "|         19|    brittany79|         Kiara Smith|     F|victoria46@yahoo.com|         Ship broker|2013-06-10|47282 Christina C...| 00286fb0|\n",
      "|         20|     smithkurt|        Richard Kent|     M|  thomas83@yahoo.com|Geographical info...|2004-10-09|USS FordFPO AA 15552| 0028f49f|\n",
      "+-----------+--------------+--------------------+------+--------------------+--------------------+----------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "verify_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "723dc987-a3a3-419b-8928-a76be0f5f2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "hist_ded_ids = ct_hist_df.select(\"device_id\").sample(.01).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c7c3179e-da9b-4400-b5fa-71830e79cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_arr2 = np.unique(hist_ded_ids.device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "77105651-e660-46b2-8ae6-98a4fa0f212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_device_ids = [i for i in unique_arr2 if i not in unique_arr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dd457d9d-a5ba-46df-b88a-d8b374c8e1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29261"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_device_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1f856a8c-b89e-4239-989c-7d7184f982fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_df = pd.DataFrame(sample_device_ids, columns=['device_id'])\n",
    "device_id_spark_df = spark.createDataFrame(array_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "83eca045-2fd8-432c-9ecf-d8a356ab69a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "device_id_spark_df.write.csv(os.environ[\"STORAGE\"]+'/cde-workshop/clickthrough/customers/data/device_ids', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0fc319be-848b-463e-bba6-7ec337f1181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch_df(max_current_cust_id):\n",
    "    \n",
    "    batch_size = random.randint(100, 1000)\n",
    "    appends = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        appends.append(fake.profile())\n",
    "        \n",
    "    appends_df = pd.DataFrame(appends)[[\"username\", \"name\", \"sex\", \"mail\", \"job\", \"birthdate\", \"address\"]]\n",
    "    appends_df['customer_id'] = list(range(max_current_cust_id, max_current_cust_id+batch_size))\n",
    "    \n",
    "    appends_df.rename({'sex': 'gender', 'job': 'position', 'mail':'email'}, axis=1)\n",
    "    \n",
    "    return appends_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ab9a9f7-c049-442c-b5d2-ebe3e0e68c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_pd_df = make_batch_df(max_current_cust_id)\n",
    "new_batch_pd_df_count = new_batch_pd_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "309894de-c4df-453d-82be-4559f2c56461",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_spark_df = spark.createDataFrame(new_batch_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "93ee6694-0bc6-4cfc-8102-b633bedbfa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_len = new_batch_pd_df_count[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "00d79982-1554-4e46-8ecc-7ec09922132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "in_device_id_spark_df = spark.read.csv(os.environ[\"STORAGE\"]+'/cde-workshop/clickthrough/customers/data/device_ids', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "551ef2ea-838a-4724-a8cd-0eb9ed4f73a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_device_slice = in_device_id_spark_df.limit(new_batch_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b976d9ce-029e-4d13-8873-ba1a35f2bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8e4da47c-9475-436d-84b9-45bbb588eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_device_slice = in_device_slice.withColumn(\"mono_id\",monotonically_increasing_id() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "47d54ec5-2e4c-42b9-b29b-daa9fce62c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_spark_df = new_batch_spark_df.withColumn(\"mono_id\",monotonically_increasing_id() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eff37a50-6695-40d9-8fe8-7bf699793a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_spark_append_df = new_batch_spark_df.join(in_device_slice, 'mono_id').drop('mono_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f28d0e5e-9952-49de-83d2-e863611d7b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('username', 'string'),\n",
       " ('name', 'string'),\n",
       " ('sex', 'string'),\n",
       " ('mail', 'string'),\n",
       " ('job', 'string'),\n",
       " ('birthdate', 'date'),\n",
       " ('address', 'string'),\n",
       " ('customer_id', 'bigint'),\n",
       " ('device_id', 'string')]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_batch_spark_append_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "30ddce7e-32e6-4e03-a7fb-eff9f2075fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('customer_id', 'string'),\n",
       " ('username', 'string'),\n",
       " ('name', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('email', 'string'),\n",
       " ('occupation', 'string'),\n",
       " ('birthdate', 'string'),\n",
       " ('address', 'string')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "be42d24b-4081-4a2e-8d98-6a5cab8e1731",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o517.saveAsTable.\n: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.kudu.KuduStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:347)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:486)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:445)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:445)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:319)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:246)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:245)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:299)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:443)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:92)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:123)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:714)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:714)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:515)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:281)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1287)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$15.applyOrElse(Analyzer.scala:1204)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$15.applyOrElse(Analyzer.scala:1167)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1167)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1133)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:172)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:193)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:197)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:197)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:600)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:649)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.kudu.KuduStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:303)\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:342)\n\t... 80 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.kudu.KuduStorageHandler\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:298)\n\t... 81 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_203/1212166453.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcustomers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'default.CUSTOMERS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o517.saveAsTable.\n: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.kudu.KuduStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:347)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.convertHiveTableToCatalogTable(HiveClientImpl.scala:486)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$3(HiveClientImpl.scala:445)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTableOption$1(HiveClientImpl.scala:445)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:319)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:246)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:245)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:299)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTableOption(HiveClientImpl.scala:443)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable(HiveClient.scala:90)\n\tat org.apache.spark.sql.hive.client.HiveClient.getTable$(HiveClient.scala:89)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:92)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:123)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTable$1(HiveExternalCatalog.scala:714)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:714)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:515)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)\n\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:65)\n\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:281)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1250)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1287)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1286)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$15.applyOrElse(Analyzer.scala:1204)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$15.applyOrElse(Analyzer.scala:1167)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1167)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1133)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:209)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:172)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:193)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:197)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:197)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:600)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:649)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:565)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Error in loading storage handler.org.apache.hadoop.hive.kudu.KuduStorageHandler\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:303)\n\tat org.apache.hadoop.hive.ql.metadata.Table.getStorageHandler(Table.java:342)\n\t... 80 more\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.kudu.KuduStorageHandler\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.hadoop.hive.ql.metadata.HiveUtils.getStorageHandler(HiveUtils.java:298)\n\t... 81 more\n"
     ]
    }
   ],
   "source": [
    "customers_df.write.mode(\"overwrite\").saveAsTable('default.CUSTOMERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6616d-1463-41e5-bc5f-8626c3aaed72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553221f7-9e8d-4a63-b321-12837c4e9534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b695d66-13e8-4daa-871e-d692c1cbb318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d9bfe-d15a-4690-bb93-16a8b0736d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_batch_spark_append_df.union(customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca550aa-5bdf-47a6-b4d7-3a9877fc3e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26404d59-1a06-4af8-8e16-172fc8cbe5c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
